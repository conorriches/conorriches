{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/birdwatching/","result":{"data":{"markdownRemark":{"html":"<p>Let's face it, birdwatching takes time. A <strong>whole lot</strong> of time.</p>\n<p>Annoyingly for us, birds have evolved not make appearances when there's a big mammal eagerly waiting for them, so what better to do than to automate the hell out of it? Raspberry Pi's rarely move by their own accord, so they're perfect for setting up without spooking wildlife.</p>\n<h2>Project overview</h2>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAQL/xAAWAQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhADEAAAAZutYZlHE//EABkQAAMAAwAAAAAAAAAAAAAAAAABAgMSIv/aAAgBAQABBQIcjoTW+Z9zKs//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFX/8QAFhEBAQEAAAAAAAAAAAAAAAAAABES/9oACAECAQE/AYy//8QAGRAAAgMBAAAAAAAAAAAAAAAAABEBAiEQ/9oACAEBAAY/Au7GDqNn/8QAGxABAAIDAQEAAAAAAAAAAAAAAQARITFRYaH/2gAIAQEAAT8hAOkSjDe9RrzYz4Cj2s9gfaf/2gAMAwEAAgADAAAAEDQ//8QAFxEAAwEAAAAAAAAAAAAAAAAAARARMf/aAAgBAwEBPxAXV//EABkRAAIDAQAAAAAAAAAAAAAAAAABESExcf/aAAgBAgEBPxBJF6cH/8QAGhABAAMBAQEAAAAAAAAAAAAAAQARITFBcf/aAAgBAQABPxBQQCvs0uGnCwNaDpBUsIWvdgLA2afDnYpYFaI9n//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"A bluetit, caught with the Raspberry Pi HQ camera\"\n        title=\"A bluetit, caught with the Raspberry Pi HQ camera\"\n        src=\"/static/23c9669aaa8fbf1c3f265fd5a855bef3/1c72d/blue.jpg\"\n        srcset=\"/static/23c9669aaa8fbf1c3f265fd5a855bef3/a80bd/blue.jpg 148w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/1c91a/blue.jpg 295w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/1c72d/blue.jpg 590w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/a8a14/blue.jpg 885w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/fbd2c/blue.jpg 1180w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/eea4a/blue.jpg 1280w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">A bluetit, caught with the Raspberry Pi HQ camera</figcaption>\n  </figure></p>\n<p>We are going to use a Raspberry Pi with an HQ Camera, paired with TensorFlow Lite machine learning to identify when there's a bird in front of the camera, and take a photo when it sees one. Optionally, we can use a touchscreen to make the process of using the camera much easier.</p>\n<p>The Raspberry Pi HQ camera lens has a fixed focus, so it should be focused on a set point, such as a birdbath, feeder, or table where birds are likely to visit when left alone. You can also just leave the birbcam on the (dry) grass and place some seed in front of it and you'll be sure to attract at least a pigeon.</p>\n<p>We are going to use the new <a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#libcamera-and-libcamera-apps\">libcamera</a> stack - so make sure you have a recent version of Raspberry Pi OS installed and everything is up to date.</p>\n<div style=\"clear:both\"></div>\n<h2>Parts required</h2>\n<ul>\n<li>A Raspberry Pi, with up-to-date OS, connected to</li>\n<li>Pi HQ Camera, with a</li>\n<li>16mm lens, mounted on a</li>\n<li>tripod</li>\n<li>optionally, a <a href=\"https://thepihut.com/products/3-5-ips-dpi-capacitive-touchscreen-display-for-raspberry-pi\">touchscreen display</a> (more on this later)</li>\n</ul>\n<p>There's no coding needed to get it up and running, but some proficiency with Raspberry Pi will come in handy.</p>\n<p><strong>Note: You don't need to use the HQ camera if you're on a budget - a standard camera module will functionally work the same, just not the same photo quality!</strong></p>\n<h2>Prerequisites</h2>\n<p>This guide doesn't cover everything, so here's some good things to start with:</p>\n<ul>\n<li><a href=\"https://www.raspberrypi.com/software/\">Setting up a Raspberry Pi</a></li>\n<li><a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#installing-a-raspberry-pi-camera\">Installing a Raspberry Pi camera</a></li>\n<li><a href=\"https://roboticsbackend.com/enable-ssh-on-raspberry-pi-raspbian/\">Enabling ssh</a></li>\n<li>Basic linux commands</li>\n</ul>\n<hr>\n<h2>Step Zero - Optional - Get the touchscreen working</h2>\n<p>I chose a 3.5inch display which plugs into the 40 pin GPIO header as we won't be using the GPIO for anything else and it means the entire camera and screen construction will nicely hold together as one piece. We've essentially just made a really fancy digital camera!</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 570px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAAMBAQAAAAAAAAAAAAAAAAMEBQEC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAIB/9oADAMBAAIQAxAAAAGfkdrczXlK3RJrQB//xAAZEAEAAwEBAAAAAAAAAAAAAAACAAEDEiL/2gAIAQEAAQUC7dz3cT5WTIlPO5ZdvTIKHI9DOhX/xAAXEQEAAwAAAAAAAAAAAAAAAAABABAh/9oACAEDAQE/AYOX/8QAFREBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQIBAT8BI//EAB4QAAICAQUBAAAAAAAAAAAAAAARAQISITFBQpGx/9oACAEBAAY/Apx+CnL0mIaLVtbsblteRoQoP//EABwQAAMBAAIDAAAAAAAAAAAAAAABESExUaHB4f/aAAgBAQABPyGxtbL9DjBk+cr0PQmTi7MFshpoMarvRMIgcNtdjLMvJpTXdZ//2gAMAwEAAgADAAAAEHTPgP/EABcRAQEBAQAAAAAAAAAAAAAAAAERABD/2gAIAQMBAT8QWZ4o9//EABgRAQADAQAAAAAAAAAAAAAAAAEAEBEx/9oACAECAQE/EARDeza//8QAHRABAAICAwEBAAAAAAAAAAAAAQARIVExQcFhkf/aAAgBAQABPxAGKLgFaHtWvkbjTQyzWIopSBPExcOFmjUItSx0ldwoba6PKgj75QUsbrWxCfkXWyxkb48n/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"The 3.5&quot; DPI Display working on the Pi - not required but makes it way easier!\"\n        title=\"The 3.5&quot; DPI Display working on the Pi - not required but makes it way easier!\"\n        src=\"/static/5596a2dd780efcb287cf1797506bb30e/882a4/screen.jpg\"\n        srcset=\"/static/5596a2dd780efcb287cf1797506bb30e/a80bd/screen.jpg 148w,\n/static/5596a2dd780efcb287cf1797506bb30e/1c91a/screen.jpg 295w,\n/static/5596a2dd780efcb287cf1797506bb30e/882a4/screen.jpg 570w\"\n        sizes=\"(max-width: 570px) 100vw, 570px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">The 3.5&quot; DPI Display working on the Pi - not required but makes it way easier!</figcaption>\n  </figure></p>\n<p>Getting the touchscreen working relies on following manufacturer guidelines, for <a href=\"https://thepihut.com/products/3-5-ips-dpi-capacitive-touchscreen-display-for-raspberry-pi\">the one I'm using</a>, I followed <a href=\"https://www.waveshare.com/wiki/3.5inch_DPI_LCD\">the instructions on their Wiki</a>.</p>\n<p>I found the screen was extremely dim, so followed their advice on setting brightness to maximum using PWM. I found the screen kept resetting to the dim brightness on reboot, so I saved the command inside of <code class=\"language-text\">/etc/rc.local</code> which gets run on boot and will auto-set brightness every time:</p>\n<ul>\n<li>Run <code class=\"language-text\">sudo nano /etc/rc.local</code> to edit the file</li>\n<li>Enter <code class=\"language-text\">gpio -g pwm 18 100</code> <strong>before</strong> the line saying <code class=\"language-text\">exit 0</code></li>\n</ul>\n<h3>Making the screen comfortable to use</h3>\n<p>Things will be tiny, so let's make them more usable!</p>\n<ul>\n<li>Using the Main Menu (Raspberry Pi logo top left of the screen), select Preferences, Appearance Settings.</li>\n<li>Select the Defaults tab</li>\n<li>Select the Small Screens option, and everything should resize nicely!</li>\n</ul>\n<div style=\"clear:both\"></div>\n<h2>Step one - get the camera working and set up</h2>\n<p>The main bit of the project is the camera, so install the HQ camera module as per the guidance, and boot the Raspberry Pi.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBP/EABYBAQEBAAAAAAAAAAAAAAAAAAIBA//aAAwDAQACEAMQAAABmnhtHCWf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAIREgETFCH/2gAIAQEAAQUCkZYWrG1MHR5aT//EABcRAAMBAAAAAAAAAAAAAAAAAAEDEGH/2gAIAQMBAT8BK9n/xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPwEZ/8QAGhABAAIDAQAAAAAAAAAAAAAAAQARAhAhgf/aAAgBAQAGPwKDrlyjE9lqz//EABsQAAMBAAMBAAAAAAAAAAAAAAABESExUWHB/9oACAEBAAE/IbcfL02xOvVdRdL60dpMegFbSn//2gAMAwEAAgADAAAAEKc//8QAFhEAAwAAAAAAAAAAAAAAAAAAAAEh/9oACAEDAQE/EEKin//EABcRAAMBAAAAAAAAAAAAAAAAAAABEUH/2gAIAQIBAT8QwaIf/8QAHRABAAIBBQEAAAAAAAAAAAAAAQARITFRYYGhwf/aAAgBAQABPxBZxBcXtEIfWGwbYpAHGtz2LMOo+sRja8gNc1UXrPQlYO5//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"bird.exe has stopped unexpectedly\"\n        title=\"bird.exe has stopped unexpectedly\"\n        src=\"/static/d9fea988834fadb2bff7bd72e909ef2d/1c72d/water.jpg\"\n        srcset=\"/static/d9fea988834fadb2bff7bd72e909ef2d/a80bd/water.jpg 148w,\n/static/d9fea988834fadb2bff7bd72e909ef2d/1c91a/water.jpg 295w,\n/static/d9fea988834fadb2bff7bd72e909ef2d/1c72d/water.jpg 590w,\n/static/d9fea988834fadb2bff7bd72e909ef2d/a8a14/water.jpg 885w,\n/static/d9fea988834fadb2bff7bd72e909ef2d/72e01/water.jpg 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">bird.exe has stopped unexpectedly</figcaption>\n  </figure></p>\n<p>If you don't have a touchscreen, plug in a monitor, mouse and keyboard to the Pi so we can test if the camera works.</p>\n<p>To do this, use the pre-installed\n<a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#libcamera-hello\">libcamera-hello</a> by running <code class=\"language-text\">libcamera-hello -t 0</code></p>\n<ul>\n<li>This will indefinitely show the camera output to the desktop screen</li>\n<li>Play with the camera!</li>\n<li>This is a really fun time to see how the lens works, play with adjustment, focus, light levels etc.</li>\n</ul>\n<p>Save the captured image by running <a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#libcamera-jpeg\">libcamera-jpeg</a> by running <code class=\"language-text\">libcamera-jpeg -o test.jpg</code>. Practice taking photos and viewing the saved image on the Pi.</p>\n<div style=\"clear:both\"></div>\n<h3>Copying images from the Pi to another device</h3>\n<p>To really enjoy and share the photos you take with <code class=\"language-text\">libcamera-jpeg</code> you'll want to copy them to your computer or device.</p>\n<h3>Backup to the cloud</h3>\n<p><code class=\"language-text\">rsync</code> is a really useful tool to sync files from one place to another. You can then use <code class=\"language-text\">rclone</code> to push photos to places like Google Drive or Dropbox.\nFollow the guide here on getting <code class=\"language-text\">rsync</code> and <code class=\"language-text\">rclone</code> set up:\n<a href=\"https://raspberrypi-guide.github.io/filesharing/file-synchronisation-rsync-rclone#uploading-data-to-the-cloud\">https://raspberrypi-guide.github.io/filesharing/file-synchronisation-rsync-rclone#uploading-data-to-the-cloud</a></p>\n<h3>Locally transferring photos over FTP</h3>\n<p>You'll need the Pi and other device connected to the same WiFi router, have SSH enabled on the Pi, and a client on your other device:</p>\n<ul>\n<li>On PC, you can install FileZilla</li>\n<li>On mobile, you can install a SSH/SFTP app</li>\n</ul>\n<p>Once installed:</p>\n<ul>\n<li>Set up a connection to the Pi using the <a href=\"https://www.techworked.com/find-ip-address-of-headless-raspberry-pi/\">Raspberry Pi's IP address</a>. The username/password is the same as when you SSH into the Pi, default is <code class=\"language-text\">pi</code> / <code class=\"language-text\">raspberry</code></li>\n<li>Select SFTP</li>\n<li>You should see a file listing when you connect</li>\n</ul>\n<p>Practice copying over photos, it's a bit manual but you're now up and running.</p>\n<h3>Manually doing it with SCP</h3>\n<p>If you don't want to pull photos and instead want to push them, you can use <code class=\"language-text\">scp</code> to <strong>s</strong>ecurely <strong>c</strong>o<strong>p</strong>y photos from the Pi to another device.</p>\n<h2>Step two - install TensorFlow Lite</h2>\n<p>The Raspberry Pi camera is more like a video camera than a still camera, so we need a way to trigger the script to take static photographs. Instead of using general motion detection, we will specifically detect birds so we don't need to filter through thousands of images of leaves moving.</p>\n<p>The first step is to <a href=\"https://lindevs.com/install-precompiled-tensorflow-lite-on-raspberry-pi/\">install TensorFlow Lite following the instructions</a>.</p>\n<p>TensorFlow Lite is based on machine learning and can be used to identify when there's a bird in front of the camera. It doesn't know what a bird is yet, but we can later on give it some models that describe the concept of a bird, and it'll be able to do the magic needed to then use those models to say when it can see a bird.</p>\n<h2>Step three - detection software with libcamera-apps</h2>\n<p>As you'll have seen, <code class=\"language-text\">libcamera-apps</code> come pre-installed with Raspberry Pi OS, so we can just use <a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#libcamera-detect\">libcamera-detect</a> to open the camera up, and plug into TensorFlow Lite to identify when a bird is on screen.</p>\n<p>While <code class=\"language-text\">libcamera-apps</code> comes with the OS, <code class=\"language-text\">libcamera-detect</code> does not come pre-built so we will need to build it, folding in our now-installed TensorFlow Lite installation.</p>\n<ul>\n<li>\n<p>Rebuild libcamera-apps</p>\n<ul>\n<li>Start by <a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#building-libcamera-and-libcamera-apps\">rebuilding libcamera-apps without libcamera</a></li>\n<li>Run <code class=\"language-text\">sudo apt install -y libcamera-dev libepoxy-dev libjpeg-dev libtiff5-dev</code></li>\n</ul>\n</li>\n<li>\n<p>Make sure you continue on to build <a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#building-libcamera-apps\">libcamera-apps</a></p>\n<p>Install the dependencies: <code class=\"language-text\">sudo apt install -y cmake libboost-program-options-dev libdrm-dev libexif-dev</code></p>\n<p>Then you'll need to get prepared to build by downloading the code for <code class=\"language-text\">libcamera-apps</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">cd\ngit clone https://github.com/raspberrypi/libcamera-apps.git\ncd libcamera-apps\nmkdir build\ncd build</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>\n<p>Build it real good</p>\n<ul>\n<li>Run the command:</li>\n</ul>\n</li>\n</ul>\n<p><code class=\"language-text\">cmake .. -DENABLE_DRM=1 -DENABLE_X11=1 -DENABLE_QT=1 -DENABLE_OPENCV=0 -DENABLE_TFLITE=1</code></p>\n<ul>\n<li>This last flag means that your installed TensorFlow Lite is enabled with the build, meaning that <code class=\"language-text\">libcamera-detect</code> will be able to use it.</li>\n<li>Finish the process off\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">make -j4  # use -j1 on Pi 3 or earlier devices\nsudo make install\nsudo ldconfig</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n</li>\n</ul>\n<p>Breathe - that's the tricky part done!</p>\n<h2>Step Four - when is a bird a bird?</h2>\n<p>So far we have most of the pieces in place but the software still doesn't know what a bird is.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 77.02702702702703%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQBAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABWlcTYLip/8QAGxAAAgMAAwAAAAAAAAAAAAAAAQIAAwQREiL/2gAIAQEAAQUCOhg2U+iol6hLaupfif/EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/AUf/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARL/2gAIAQIBAT8BjT//xAAbEAEAAgIDAAAAAAAAAAAAAAABAAIRIRAxQf/aAAgBAQAGPwJzheo1ZpjU8lS2TfH/xAAbEAACAgMBAAAAAAAAAAAAAAAAAREhMUFhUf/aAAgBAQABPyHp7BVKk7gkaB2bfRPCpi5I+n//2gAMAwEAAgADAAAAEHjf/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERUf/aAAgBAwEBPxBovD//xAAXEQADAQAAAAAAAAAAAAAAAAAAARFR/9oACAECAQE/EGiI0//EAB0QAQACAgIDAAAAAAAAAAAAAAEAESFhMaFxgcH/2gAIAQEAAT8QKaqsNDS5x5imKYDCvHPyDZbsQ3GDOxddxNlaGgd4NdxVVJ6Z/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"The setup using a Pi mount on the tripod\"\n        title=\"The setup using a Pi mount on the tripod\"\n        src=\"/static/ad3fd60729ead300e644e1deb21fb5d6/1c72d/camera.jpg\"\n        srcset=\"/static/ad3fd60729ead300e644e1deb21fb5d6/a80bd/camera.jpg 148w,\n/static/ad3fd60729ead300e644e1deb21fb5d6/1c91a/camera.jpg 295w,\n/static/ad3fd60729ead300e644e1deb21fb5d6/1c72d/camera.jpg 590w,\n/static/ad3fd60729ead300e644e1deb21fb5d6/a8a14/camera.jpg 885w,\n/static/ad3fd60729ead300e644e1deb21fb5d6/9100a/camera.jpg 959w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">The setup using a Pi mount on the tripod</figcaption>\n  </figure></p>\n<p>Therefore, we need a model which describes what a bird is, and fortunately the hard work is done for us as online there exists a library of common objects which has pre-trained models which we can download.</p>\n<p><a href=\"https://cocodataset.org\">Common Objects In Context</a> (COCO) is a machine learning dataset of common objects (including birds) that we can use without having to train up our own model of a bird. From exploring the dataset, it looks like it will work perfectly.</p>\n<p>For example, this image has multiple items identified:\n<a href=\"https://cocodataset.org/#explore?id=12805\">https://cocodataset.org/#explore?id=12805</a></p>\n<p>The context is nothing if not diverse either:\n<a href=\"https://cocodataset.org/#explore?id=43511\">https://cocodataset.org/#explore?id=43511</a></p>\n<p>You'll want to download the models and labels file somewhere sensible, I've put them in <code class=\"language-text\">/home/pi/models</code>:\n<a href=\"https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md#option-1-using-googles-sample-tflite-model\">Follow the instructions here</a></p>\n<p>Create a file called <code class=\"language-text\">object_detect_tf.json</code> and paste the following in:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">{\n    \"object_detect_tf\":\n    {\n        \"number_of_threads\" : 2,\n        \"refresh_rate\" : 10,\n        \"confidence_threshold\" : 0.5,\n        \"overlap_threshold\" : 0.5,\n        \"model_file\" : \"/home/pi/models/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29/detect.tflite\",\n        \"labels_file\" : \"/home/pi/models/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29/labelmap.txt\",\n        \"verbose\" : 1\n    }\n}</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>This will be used as a \"post process file\", which means that it's used when the camera is running. It has a model and labels file, so it can understand what it sees and match it up with a label. There are some parameters which you can play with, but these worked well for me.</p>\n<h2>Step Five - let's go!</h2>\n<p>So far we have:</p>\n<ul>\n<li>Installed a camera and checked it works</li>\n<li>Copied photos over to another device for viewing</li>\n<li>Installed TensorFlow Lite which does the fancy object detection bit</li>\n<li>Built <code class=\"language-text\">libcamera-detect</code> which connects to the camera and runs the output through TensorFlow Lite</li>\n</ul>\n<p>We will now run the command that will:</p>\n<ul>\n<li>start the camera</li>\n<li>start watching for birds</li>\n<li>take a photo when one is identified</li>\n</ul>\n<p>That's done by running the following command:\n<code class=\"language-text\">libcamera-detect -t 0 -o ~/birb%04d.jpg --lores-width 400 --lores-height 300 --post-process-file object_detect_tf.json --info-text \"Detecting Birds!\" --object bird</code></p>\n<p>You can tweak the parameters by looking at <a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#libcamera-detect\">the libcamera-detect docs</a> and <a href=\"https://www.raspberrypi.com/documentation/accessories/camera.html#common-command-line-options\">common command line options</a>:</p>\n<ul>\n<li><code class=\"language-text\">-t 0</code>\n<ul>\n<li>time is zero</li>\n</ul>\n</li>\n<li><code class=\"language-text\">-o birb%04d.jpg</code>\n<ul>\n<li>Output file is a 4 digit number that increments. This means you don't have to manually name each image - it does it for you. <strong>Note:</strong> the counter resets from 0 when you run the script so will overwrite images with the same name.</li>\n</ul>\n</li>\n<li><code class=\"language-text\">--lores-width</code>/<code class=\"language-text\">--lores-height</code>\n<ul>\n<li>This is the image sized used by TensorFlow to detect birds. When it recognises a bird, it'll then take a full scale photo.</li>\n</ul>\n</li>\n<li><code class=\"language-text\">--post-process-file object_detect_tf.json</code>\n<ul>\n<li>we are telling it what to do</li>\n</ul>\n</li>\n<li><code class=\"language-text\">--info-text Detecting</code>\n<ul>\n<li>Shows in the info bar title on the window so we can see it's working</li>\n</ul>\n</li>\n<li>--object bird\n<ul>\n<li>telling it what to look out for! You can find more labels in the labels file if you want to spot other things</li>\n</ul>\n</li>\n</ul>\n<h2>Step Six - Optional - Touchscreen shortcuts</h2>\n<p>If you have a touchscreen, you can use desktop shortcuts to run commands such as</p>\n<ul>\n<li><code class=\"language-text\">libcamera-hello -t 30</code> as a viewfinder and way of setting focus when on site</li>\n<li><code class=\"language-text\">libcamera-detect ...</code> to run the bird spotting program!</li>\n</ul>\n<h3>Create a shell script</h3>\n<p>To keep things separated, we will create a shell script in our home directory (<code class=\"language-text\">/home/pi</code>) that will run the <code class=\"language-text\">libcamera-detect</code> command. Create the file by running <code class=\"language-text\">nano ~/detect.sh</code> and paste into it the following:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">#!/bin/bash\n\nDATE=$(date +\"%Y-%m-%d_%H%M\")\nmkdir $DATE\nlibcamera-detect -t 0 --lores-width 400 --lores-height 300 --post-process-file object_detect_tf.json --object bird -o birds/$DATE/%04d.jpg</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<ul>\n<li>This will get run the <code class=\"language-text\">libcamera-detect</code> application</li>\n<li>It will save the images with a file name that starts with the current date and ends with the increment number, e.g. <code class=\"language-text\">2020-04-14_1015_0001.jpg</code></li>\n<li>That way we won't get any files overwritten even if there are two photos taken in the same minute, and even if we stop and restart the script.</li>\n<li>Note: the date is generated once when the script is started, so it won't update the time and date on each photo taken, but they will still be incremented</li>\n</ul>\n<p>Now do the same for <code class=\"language-text\">libcamera-hello</code> - create a file called <code class=\"language-text\">viewfinder.sh</code> with the <code class=\"language-text\">libcamera-hello -t 30</code> command inside.\nNext we will get these shell scripts to run when a desktop shortcut is pressed.</p>\n<h3>Creating desktop shortcuts</h3>\n<ul>\n<li>Navigate to the Desktop directory <code class=\"language-text\">cd ~/Desktop</code> - anything saved here will show up on the Desktop.</li>\n<li>Create a new file: <code class=\"language-text\">nano Detect.desktop</code></li>\n<li>Paste into it:\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">[Desktop Entry]\nComment=Preview\nTerminal=true\nName=Detect\nExec=lxterminal -e \"/home/pi/detect.sh\"\nType=Application\nIcon=/usr/share/raspberrypi-artwork/raspberry-pi-logo-small.png</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>Make them run with one press\n<ul>\n<li>Open the file browser</li>\n<li>Edit menu -> Preferences</li>\n<li>Select the first checkbox \"Open files with single click\"</li>\n<li>Select the option lower down not to ask options when opening executables</li>\n</ul>\n</li>\n<li>Do the same for <code class=\"language-text\">Viewfinder</code> which will run the <code class=\"language-text\">viewfinder.sh</code> file which will in turn run <code class=\"language-text\">libcamera-hello</code></li>\n</ul>\n<p>You now have two icons on the Desktop, one will open up <code class=\"language-text\">libcamera-hello</code> and acts as a viewfinder, and the other which will open up <code class=\"language-text\">libcamera-detect</code>.</p>\n<h2>Tips on using the camera</h2>\n<ul>\n<li>Use a good tripod so the camera is steady and resistant to any wind you'll likely get - you don't want it falling over!</li>\n<li>The screws that came with the <a href=\"https://thepihut.com/products/mounting-plate-for-high-quality-camera?variant=31867507114046\">mounting plate for the HQ camera</a> are plastic, and I was really disappointed when they failed - so maybe upgrade to some brass screws and standoffs - these can be found anywhere online.</li>\n<li>Focus the camera on a specific point, maybe a pile of seed. This will ensure they're in best focus.</li>\n<li>Most of all, have fun!</li>\n<li>Share your photos!</li>\n</ul>\n<h2>Future steps...</h2>\n<p>This is just the start of a much bigger project - what we have done here works with really good results.</p>\n<p>How about:</p>\n<ul>\n<li>Making it post to Telegram when a bird is identified?</li>\n<li>Making this setup work as a security camera (hint: set <code class=\"language-text\">--object person</code>)</li>\n<li>Recording a short video when a bird is detected\n<ul>\n<li>How about getting it to pre-record 5s before?</li>\n</ul>\n</li>\n<li>Auto-cropping the image with the bird in the centre?</li>\n<li>Using OpenCV to draw on the images</li>\n<li>Could it detect the species of bird and name the file?</li>\n</ul>\n<p>There's also a load of other options on the utilities above to play around with.</p>\n<p>And this is all without having to write a script!</p>\n<p><strong>Happy birdwatching!</strong></p>","frontmatter":{"title":"BirbCam - Automatic Birdwatching with Raspberry Pi HQ Camera","featuredImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAQL/xAAWAQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhADEAAAAZutYZlHE//EABkQAAMAAwAAAAAAAAAAAAAAAAABAgMSIv/aAAgBAQABBQIcjoTW+Z9zKs//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFX/8QAFhEBAQEAAAAAAAAAAAAAAAAAABES/9oACAECAQE/AYy//8QAGRAAAgMBAAAAAAAAAAAAAAAAABEBAiEQ/9oACAEBAAY/Au7GDqNn/8QAGxABAAIDAQEAAAAAAAAAAAAAAQARITFRYaH/2gAIAQEAAT8hAOkSjDe9RrzYz4Cj2s9gfaf/2gAMAwEAAgADAAAAEDQ//8QAFxEAAwEAAAAAAAAAAAAAAAAAARARMf/aAAgBAwEBPxAXV//EABkRAAIDAQAAAAAAAAAAAAAAAAABESExcf/aAAgBAgEBPxBJF6cH/8QAGhABAAMBAQEAAAAAAAAAAAAAAQARITFBcf/aAAgBAQABPxBQQCvs0uGnCwNaDpBUsIWvdgLA2afDnYpYFaI9n//Z","aspectRatio":1.3333333333333333,"src":"/static/23c9669aaa8fbf1c3f265fd5a855bef3/14b42/blue.jpg","srcSet":"/static/23c9669aaa8fbf1c3f265fd5a855bef3/f836f/blue.jpg 200w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/2244e/blue.jpg 400w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/14b42/blue.jpg 800w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/47498/blue.jpg 1200w,\n/static/23c9669aaa8fbf1c3f265fd5a855bef3/ec6c5/blue.jpg 1280w","sizes":"(max-width: 800px) 100vw, 800px"}}}}}},"pageContext":{"slug":"/blog/birdwatching/"}},"staticQueryHashes":["3682878853","3729204765"]}